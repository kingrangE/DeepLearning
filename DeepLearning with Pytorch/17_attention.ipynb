{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["cu_zSO-U2BjF"],"authorship_tag":"ABX9TyNq3bZZzkDeQsSfG2mdZOZM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Attention\n","### seq2seq과의 차이\n","- seq2seq은 encoder에서 **context vector** 라고 부르는 **하나의 고정된 벡터 표현**으로 압축하고 decoder에서는 이 **context vector**를 이용하여 **output sequence**를 만들어냅니다.\n","  - 이러한 seq2seq은 2가지 문제를 갖습니다.\n","    - 고정된 하나의 context vector로 압축하는 과정에서 정보의 손실이 발생합니다.\n","    - RNN의 고질적 문제인 gradient vanish 문제가 발생합니다.\n","  - 이 2가지 문제는 input sentence가 길어질 때, quality of translation이 떨어지는 현상으로 나타납니다.\n","- Attention은 input sequence가 길어질 때에도 quality of output을 떨어뜨리지 않게 해주는 기법입니다.\n","\n","### Attention IDEA\n","- Decoder에서 output을 예측하는 timestep마다 encoder에서의 전체 input sentence를 다시 참고\n","  - 이때 참고는 전체를 동일한 weight로 하는 것이 아닌 output과 연관이 있는 부분을 더 집중(Attention)하여 확인합니다.\n","\n","### Attention Function\n","- Attention(Q,K,V) = Attention Value\n","  - Q : Query ( t 시점의 hidden state in decoder cell )\n","  - K : Keys ( 모든 시점의 hidden state in encoder cell )\n","  - V : Values ( 모든 시점의 hidden stae in encoder cell )\n","- Attention은 주어진 Q(Query)에 대해서 모든 K(Key)와의 유사도를 구합니다. 이 유사도를 K와 매핑된 V(Value)에 반영해줍니다. 그리고 유사도가 반영된 V를 모두 더하여 결과 Attention Value를 Return 합니다.\n"],"metadata":{"id":"cu_zSO-U2BjF"}},{"cell_type":"markdown","source":["## Dot-Product Attention\n","- Attention에는 다양한 종류가 존재하는데 가장 수식적으로 쉬운 Attention 방식이 Dot-Product Attention입니다.\n","1. Attention Score 구하기\n","  - Decoder는 원래 t에서 t-1에서의 hidden state와 t-1에서의 output이 input으로 필요합니다.\n","  - Attention Mechanism에서는 추가적으로 output 예측을 위해 Attention value가 필요합니다.\n","    - 이 Attention value를 $a_t$ 라고 하겠습니다.\n","  - $a_t$를 구하기 위해서는 Attention Score를 구해야 합니다.\n","    - Attention Score는 encoder의 모든 hidden state 각각이 decoder 현 시점($t$)에서의 hidden state($s_t$)와 얼마나 유사한지를 판단하는 Score입니다.\n","    - 이것을 구하기 위해 Dot-Product Attention에서는 $s_t$를 transpose하고 각 encoder의 hidden state와 내적(Dot-Product)을 수행합니다. (내적이므로 결과는 스칼라입니다. )\n","    - 따라서 스코어 함수는 아래와 같습니다.\n","      - $score(s_t,h_i)={s^T_t}h_i$\n","    - 스코어 함수를 통해 구한 decoder의 $s_t$와 모든 encoder hidden state($h_i$)와의 스코어 모음 값들을 $e^t$라고 정의하겠습니다.\n","      - $e^t = [{s_^T_t}h_1, {s_^T_t}h_2,...,{s_^T_t}h_t] $\n","2. softmax를 이용하여 attention distribution을 구하기\n","    - $e^t$에 소프트 맥스 함수를 적용하면, 모든 값을 합하면 1이 되는 probability distribution을 얻을 수 있습니다.\n","      - 이 probability distribution을 attention distribution이라고 합니다.\n","        - 또한, 여기서 각각의 값을 Attention Weight라고 부릅니다.\n","      - Attention distribution을 $a^t$라고 부르고 식은 아래와 같습니다.\n","        - $α^t = softmax(e^t)$\n","3. Attention weight와 hidden state($h_i$)를 weighted sum 하여 Attention Value 구하기\n","  - Attention의 최종 결과를 얻기 위해 각 encoder의 hidden state($h_i$)와 Attention Weight들을 곱하고 더합니다. (Weighted Sum)\n","  - Attention의 최종 결과 식은 아래와 같습니다.\n","    - $a_t = {sum^N_{i=1}}{\\alpha_i^t}h_i$\n","  - 이 Attention Value $a_t$는 Encoder의 Context를 포함하고 있다는 의미로 Context Vector라고 부릅니다.\n","    - seq2seq에서 encoder 마지막 hidden state를 context vector라고 부르는 것과 대조됩니다. ( Attention에서는 모든 Encoder의 hidden state를 weighted sum하기 때문입니다.)\n","4. Attention Value와 Decoder의 $t$시점의 hidden state($s_t$)를 concatenate\n","  - 3에서 계산한 $a_t$를 $s_t$와 concatenate하여 $v_t$를 구합니다.\n","    - 이 $v_t$는 $\\hat{y}$ 예측 연산의 input으로 이용되어 encoder로부터 얻은 정보를 활용하여 $\\hat{y}$ 예측을 잘 할 수 있게 합니다.\n","      - 이것이 Attention Mechanism 입니다.\n","        - 모든 Encoder의 정보를 weighted sum한 $a_t$를 이용하여 output 예측에 사용함으로서, output 예측 과정에서 모든 encoder의 hidden state를 참고하게 됩니다.\n","5. 출력층 연산의 입력이 되는 $\\tilde{s_t}$를 계산합니다.\n","  - Attention Paper에서는 $v_t$를 바로 output layer로 보내지 않고 그 전에 신경망 연산을 하나 더 추가하였습니다.\n","    - 해당 연산은 아래와 같습니다.\n","      - $\\tilde{s_t}= tanh(v_t*W_c + b_c)$\n","        - 내적이 아닌 곱연산입니다.\n","        - 여기서 $W_c$는 learnable weight matrix이며, $b_c$는 bias입니다.\n","6. 최종적으로 $\\tilde{s_t}$를 output layer의 input으로 사용합니다.\n","  - 최종 결과 식은 아래와 같습니다.\n","    - $\\hat{y_t} = Softmax(W_y{\\tilde{s_t}}+ b_y)$\n"],"metadata":{"id":"ClPRTWdC9c04"}},{"cell_type":"markdown","source":["##다른 Attention\n","- Attention Score를 구하는 방식에서 Attention마다의 차이가 존재합니다.\n","1. dot-porduct (Luong Attention)\n","  - $score(s_t,h_i) = {s^T_t}{h_i}$\n","2. scaled-dot\n","  - $score(s_t,h_i) = \\frac{{s^T_t}{h_i}}{\\sqrt{n}}$\n","3. general\n","  - $score(s_t,h_i) = {s^T_t}{W_a}{h_i}$\n","4.  concat (Bahdanau Attemtion)\n","  - $score(s_t,h_i) = {W^T_a}tanh({W_b}[s_t;h_i])$\n","  - $score(s_t,h_i) = {W^T_a}tanh({W_b}{s_t}+{W_c}{h_i})$\n","5. location-base\n","  - $α_t = softmax({W_a}{s_t})$\n","    - $α_t$(attention value)산출 시에만 $s_t$를 사용하는 방식입니다.\n","- 여기서\n","  - $s_t$는 Query\n","  - $h_i$는 Key\n","  - $W_a$,$W_b$는 learnable weighted matrix\n"],"metadata":{"id":"QwuuPKk8Drr5"}},{"cell_type":"markdown","source":["## Bahdanau Attention ( concat Attention )\n","### Bahdanau Attention 정의\n","- $Attention(Q,K,V) = Attention Value$\n","```\n","Q = Query : t-1 시점의 Decoder의 hidden state\n","K = Keys : 모든 시점의 Encoder의 hidden states\n","V = Values : 모든 시점의 Encoder의 hidden states\n","```\n","\n","### Bahdanau Attention 연산\n","1. Attention Score 구하기\n","  - 우선, Bahdanau Attention에서는 encoder의 hidden states($h_1,h_2,...,h_t)$와 decoder의 hidden state($s_t$)가 같은 크기의 dimension을 가진다고 가정합니다.\n","  - 정의에 적혀있듯, Bahdanau Attention에서는 Dot-Product Attention과 다르게 Query로 t-1(한 단계 이전 시점)의 hidden state를 사용합니다.\n","    - 따라서 attention score 함수도 $s_{t-1}$을 사용하고 score함수는 아래와 같습니다.\n","      - $score(s_{t-1},h_i) ={W_a^T}tanh({W_b}{s_{t-1}}+W_c{h_i})$\n","        - 여기서 ${W_a},{W_b},{W_c}$는 모두 learnable weight matrix입니다.\n","        - 여기서 h_i는 여러 개이고 각각의 attention score를 구해야 하므로, 병렬 연산을 위해 하나의 행렬($H$)로 치환합니다.\n","        - $e^t = score(s_{t-1},H) ={W_a^T}tanh({W_b}{s_{t-1}}+W_c{H})$\n","2. Softmax를 이용하여 Attention distribution 구하기\n","  - Dot-Product Attention과 동일합니다.\n","3. Attention Weight와 hidden state ${s_{t-1}}$과 weighted sum을 통해 Attention Value 구하기\n","  - Dot-Product Attention과 동일합니다.\n","  - Attention Value = Context Vector\n","4. Context Vector로부터 ${s_t}$를 구합니다.\n","  - LSTM으로 돌아가 seq2seq에서 decoder로 사용한 LSTM 입력을 상기합니다.\n","    - LSTM은 입력으로 t-1 시점에서 전달받은 ${s_{t-1}}$과 t 시점에서의 입력 ${x_t}$를 이용하여 연산합니다.\n","  - Attention은 Context Vector와 ${x_t}$를 concatenate하여 새로운 t시점의 입력으로 사용합니다. 외에는 LSTM과 동일합니다."],"metadata":{"id":"FfxZevO7ZMEu"}},{"cell_type":"markdown","source":["# Attention 실습 ( Attention을 이용한 Translator )\n"],"metadata":{"id":"KnEWAdUwxM5l"}},{"cell_type":"markdown","source":["### 1. Data Load and preprocessing\n","-  간단한 실습용 데이터를 이용합니다.\n","-  translator를 위해서는 parallel corpus 데이터가 필요합니다. ( 두 개 이상의 언어가 병렬적으로 구성된 corpus )\n","-  data를 다운 받은 후, 압축을 풀어 kor.txt 파일을 얻고 이것을 이용합니다.\n","  - [parallel corpus data link (kor-eng.zip)](http://www.manythings.org/anki)"],"metadata":{"id":"Yr2CMoJ3K1dA"}},{"cell_type":"code","source":["# 1. Data Load and preprocessing\n","# 간단한 실습용 데이터를 이용합니다.\n","## translator를 위해서는 parallel corpus 데이터가 필요합니다. ( 두 개 이상의 언어가 병렬적으로 구성된 corpus )\n","## data를 다운 받은 후, 압축을 풀어 kor.txt 파일을 얻고 이것을 이용합니다.\n","\n","## 1-1 Data Download\n","import re\n","import os\n","import unicodedata\n","import urllib3\n","import zipfile\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import torch\n","from collections import Counter\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","!wget -c https://www.manythings.org/anki/kor-eng.zip && unzip -o kor-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qidI7zczx96V","executionInfo":{"status":"ok","timestamp":1739463130372,"user_tz":-540,"elapsed":522,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"911efeff-e804-4981-d9b2-4517da9b8515"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-13 16:12:09--  https://www.manythings.org/anki/kor-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n","HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n","\n","    The file is already fully retrieved; nothing to do.\n","\n","Archive:  kor-eng.zip\n","  inflating: _about.txt              \n","  inflating: kor.txt                 \n"]}]},{"cell_type":"code","source":["## 1-2. preprocess function\n","\n","#사용할 샘플 수를 정합니다.\n","num_samples = 4444\n","\n","def en_preprocess_sentence(sentence):\n","  sent = re.sub(r\"([?.!,?`])\",\" \\1\",sentence) #단어와 구두점 사이에 공백을 만듭니다.\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\",r\" \",sent) # 결과에서 a-z,A-Z,!,.,?를 제외하고 모두 공백으로 변환합니다.\n","  sent = re.sub(r\"\\s+\",\" \",sent) #공백이 여러 개인 경우 하나로 변환합니다.\n","  return sent\n","\n","def ko_preprocess_sentence(sentence):\n","  sent = re.sub(r\"([?.!,?`])\",\" \\1\",sentence) #단어와 구두점 사이에 공백을 만듭니다.\n","  sent = re.sub(r\"[^ㄱ-힣!.?]+\",r\" \",sent) # 결과에서 ㄱ-ㅎ,!,.,?를 제외하고 모두 공백으로 변환합니다.\n","  sent = re.sub(r\"\\s+\",\" \",sent) #공백이 여러 개인 경우 하나로 변환합니다.\n","  return sent\n","\n","def load_preprocessed_data():\n","  encoder_input,decoder_input,decoder_target = [],[],[]\n","  with open(\"kor.txt\",\"r\") as lines :\n","    for i,line in enumerate(lines):\n","      src_line, tar_line, _ = line.strip().split(\"\\t\") # 데이터는 소스와 타겟이 탭으로 구분되어 있습니다. # _는 split을 하는 경우 뒤에 주석까지 따라오기에 처리하기 위한 부분입니다.\n","      src_line = [ w for w in en_preprocess_sentence(src_line).split()]\n","\n","      tar_line = ko_preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \"+ tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","      if i== num_samples - 1 :\n","        break\n","  return encoder_input,decoder_input,decoder_target"],"metadata":{"id":"G3bIOkyRzeDg","executionInfo":{"status":"ok","timestamp":1739463130675,"user_tz":-540,"elapsed":10,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# 전처리 함수를 테스트합니다.\n","en = \"Women like men with mustaches.\"\n","ko = \"여성은 수염이 있는 남성을 좋아해.\"\n","print(\"전처리 후 en :\",en_preprocess_sentence(en))\n","print(\"전처리 후 ko :\",ko_preprocess_sentence(ko))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgKlnKhG2B5i","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":11,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"02910f88-9177-4439-e45c-71604c261897"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 후 en : Women like men with mustaches \n","전처리 후 ko : 여성은 수염이 있는 남성을 좋아해 \n"]}]},{"cell_type":"code","source":["# 데이터셋을 불러옵니다.\n","sent_en_in,sent_ko_in,sent_ko_out = load_preprocessed_data()\n","print('encoder input :',sent_en_in[:10])\n","print('decoder input :',sent_ko_in[:10])\n","print('decoder label :',sent_ko_out[:10])\n","\"\"\"\n","여기에서 앞에서 공부한 것과 다르게 이전 시점의 decoder 셀의 출력을 넣어주는 것이 아닌 이전 시점의 실제 출력 값을 넣어주는 방법을 사용합니다.\n","이 이유는 만약 훈련 과정에서 이전 decoder cell의 출력 결과가 잘못되었는데 그것을 이용하여 출력한다면 연쇄적으로 잘못된 값을 예측하며 전체적으로 훈련 시간이 길어집니다.\n","이 상황을 해결하기 위해 실제 값을 넣어주는 이러한 방식을 \"교사 강요\" 라고 합니다.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"ZbPmNZ2n2yNd","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":10,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"121daba6-9197-44de-9041-6bd5139cf62a"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["encoder input : [['Go'], ['Hi'], ['Run'], ['Run'], ['Who'], ['Wow'], ['Duck'], ['Fire'], ['Help'], ['Hide']]\n","decoder input : [['<sos>', '가'], ['<sos>', '안녕'], ['<sos>', '뛰어'], ['<sos>', '뛰어'], ['<sos>', '누구'], ['<sos>', '우와'], ['<sos>', '숙여'], ['<sos>', '쏴'], ['<sos>', '도와줘'], ['<sos>', '숨어']]\n","decoder label : [['가', '<eos>'], ['안녕', '<eos>'], ['뛰어', '<eos>'], ['뛰어', '<eos>'], ['누구', '<eos>'], ['우와', '<eos>'], ['숙여', '<eos>'], ['쏴', '<eos>'], ['도와줘', '<eos>'], ['숨어', '<eos>']]\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n여기에서 앞에서 공부한 것과 다르게 이전 시점의 decoder 셀의 출력을 넣어주는 것이 아닌 이전 시점의 실제 출력 값을 넣어주는 방법을 사용합니다.\\n이 이유는 만약 훈련 과정에서 이전 decoder cell의 출력 결과가 잘못되었는데 그것을 이용하여 출력한다면 연쇄적으로 잘못된 값을 예측하며 전체적으로 훈련 시간이 길어집니다.\\n이 상황을 해결하기 위해 실제 값을 넣어주는 이러한 방식을 \"교사 강요\" 라고 합니다.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["len(sent_en_in)"],"metadata":{"id":"ePzH43CZJm6T","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":8,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"a1a04ff8-4957-4e89-8f8f-d7c3bb73ddf2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4444"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# 단어 집합을 만들어 단어로부터 정수를 얻도록 합니다.\n","def build_vocab(sents):\n","  word_list = []\n","  for sent in sents : #전달받은 문장들 중 하나씩 가져옵니다.\n","    for word in sent : # 문장들의 단어를 하나씩 가져옵니다.\n","      word_list.append(word)\n","  # 각 단어 별 등장 빈도에 따라 정렬합니다.\n","  word_counts = Counter(word_list)\n","  vocab = sorted(word_counts,key = word_counts.get, reverse = True) #word_counts를 역순으로 정렬합니다.\n","  word_to_index = {}\n","  word_to_index['<PAD>'] =0\n","  word_to_index['<UNK>'] =1\n","\n","  for index, word in enumerate(vocab):\n","    word_to_index[word] = index + 2 # PAD, UNK가 0,1 인덱스를 가지기 때문\n","  return word_to_index"],"metadata":{"id":"eTZu7xXs4ax2","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":8,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# en, ko 각각에 대해 vocab을 만듭니다.\n","en_vocab = build_vocab(sent_en_in)\n","ko_vocab = build_vocab(sent_ko_in+sent_ko_out)\n","\n","# 각 size(단어 개수)를 확인합니다.\n","print(\"en size :\",len(en_vocab))\n","print(\"ko size :\",len(ko_vocab))\n","\n","# 내용을 확인합니다.\n","print(\"en keys (3~5) :\" ,list(en_vocab.keys())[3:6])\n","print(\"en values (3~5) :\" ,list(en_vocab.values())[3:6])\n","print(\"ko keys (3~5) :\" ,list(ko_vocab.keys())[3:6])\n","print(\"ko values (3~5) :\" ,list(ko_vocab.values())[3:6])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkA216H_AJER","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":8,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"80eda086-9d94-4324-bb49-cbaad77ff1cd"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["en size : 2602\n","ko size : 5476\n","en keys (3~5) : ['Tom', 'is', 'you']\n","en values (3~5) : [3, 4, 5]\n","ko keys (3~5) : ['<eos>', '톰은', '나는']\n","ko values (3~5) : [3, 4, 5]\n"]}]},{"cell_type":"code","source":["# {word : index} 형태로 변환합니다.\n","index_to_src = {v: k for k,v in en_vocab.items()}\n","index_to_tar = {v: k for k,v in ko_vocab.items()}\n","\n","def texts_to_sequences(sents, word_to_index):\n","  encoded_X_data = []\n","  for sent in tqdm(sents):\n","    index_sequences = []\n","    for word in sent:\n","      try :\n","        index_sequences.append(word_to_index[word])\n","      except : # 없는 단어라면 UNK 추가\n","        index_sequences.append(word_to_index['<UNK>'])\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data"],"metadata":{"id":"EYBWcCZnAdTC","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":8,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["encoder_input = texts_to_sequences(sent_en_in,en_vocab)\n","decoder_input = texts_to_sequences(sent_ko_in,ko_vocab)\n","decoder_target = texts_to_sequences(sent_ko_out,ko_vocab)\n","\n","for i,(item1,item2) in zip(range(5),zip(sent_en_in,encoder_input)):\n","  # encoder input과 english sentence 앞의 5개를 통해 정수화가 잘 되는지 확인합니다.\n","  print(f\"Index: {i}, 정 수 인 코 딩 전 : {item1}, 정 수 인 코 딩 후 : {item2}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vh1fHlMXCfz9","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":7,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"8fa2bb54-1fd4-40ec-bf48-dc27a816a0bc"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4444/4444 [00:00<00:00, 602264.60it/s]\n","100%|██████████| 4444/4444 [00:00<00:00, 628332.61it/s]\n","100%|██████████| 4444/4444 [00:00<00:00, 560434.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Index: 0, 정 수 인 코 딩 전 : ['Go'], 정 수 인 코 딩 후 : [281]\n","Index: 1, 정 수 인 코 딩 전 : ['Hi'], 정 수 인 코 딩 후 : [1470]\n","Index: 2, 정 수 인 코 딩 전 : ['Run'], 정 수 인 코 딩 후 : [1004]\n","Index: 3, 정 수 인 코 딩 전 : ['Run'], 정 수 인 코 딩 후 : [1004]\n","Index: 4, 정 수 인 코 딩 전 : ['Who'], 정 수 인 코 딩 후 : [89]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# 길이 일치를 위한 패딩 함수를 구현합니다.\n","def pad_sequences(sentences, max_len=None):\n","  if max_len is None :\n","    max_len = max([len(sentence) for sentence in sentences])\n","  features = np.zeros((len(sentences),max_len),dtype=int)\n","  for index,sentence in enumerate(sentences):\n","    if len(sentence) != 0:\n","      features[index,:len(sentence)] = np.array(sentence)[:max_len]\n","  return features"],"metadata":{"id":"GTda3oY4DJ3S","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":6,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# 패딩으로 전체 길이를 일치시킵니다.\n","encoder_input = pad_sequences(encoder_input)\n","decoder_input = pad_sequences(decoder_input)\n","decoder_target = pad_sequences(decoder_target)"],"metadata":{"id":"jCIYjuJoGqS0","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":6,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# 데이터 shape을 확인합니다. (1 차이는 <sos> <eos> 입니다.)\n","print(' 인 코 더 의 입 력 의 크 기 (shape) :',encoder_input.shape)\n","print(' 디 코 더 의 입 력 의 크 기 (shape) :',decoder_input.shape)\n","print(' 디 코 더 의 레 이 블 의 크 기 (shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHv6RTc8G1Si","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":6,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"d1b338ee-64d1-4b0d-a5a3-ad6b3a5e4204"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":[" 인 코 더 의 입 력 의 크 기 (shape) : (4444, 9)\n"," 디 코 더 의 입 력 의 크 기 (shape) : (4444, 10)\n"," 디 코 더 의 레 이 블 의 크 기 (shape) : (4444, 10)\n"]}]},{"cell_type":"code","source":["# 테스트 데이터 분리 전, 데이터를 섞습니다.\n","indices = np.arange(encoder_input.shape[0]) # 전체 개수만큼 리스트를 생성합니다.\n","np.random.shuffle(indices) # 섞습니다.\n","encoder_input = encoder_input[indices] # idx를 이용하여 순서를 섞습니다.\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"Lkl4hg-xHArG","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":6,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# 데이터를 분할합니다. 6:2:2\n","test_size = int(num_samples*0.1)\n","train_size = num_samples-test_size*2 # val, test 각각 빼줍니다.\n","\n","encoder_input_train = encoder_input[:train_size]\n","encoder_input_test = encoder_input[train_size:train_size+test_size]\n","encoder_input_val = encoder_input[train_size+test_size:]\n","\n","decoder_input_train = decoder_input[:train_size]\n","decoder_input_test = decoder_input[train_size:train_size+test_size]\n","decoder_input_val = decoder_input[train_size+test_size:]\n","\n","decoder_target_train = decoder_target[:train_size]\n","decoder_target_test = decoder_target[train_size:train_size+test_size]\n","decoder_target_val = decoder_target[train_size+test_size:]"],"metadata":{"id":"-uzB3AUbHvIA","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# 분할 후 size를 확인합니다.\n","print('train source data :',encoder_input_train.shape)\n","print('train target data :',decoder_input_train.shape)\n","print('train target label :',decoder_target_train.shape)\n","print('test source data :',encoder_input_test.shape)\n","print('test target data :',decoder_input_test.shape)\n","print('test target label :',decoder_target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iukg5NJPI-nd","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"2980bed5-f7ed-4521-91ac-1dd274c42f2c"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["train source data : (3556, 9)\n","train target data : (3556, 10)\n","train target label : (3556, 10)\n","test source data : (444, 9)\n","test target data : (444, 10)\n","test target label : (444, 10)\n"]}]},{"cell_type":"markdown","source":["### 2. machine translator 제작"],"metadata":{"id":"poVE7Q7FLUGf"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","embedding_dim =256\n","hidden_units = 256"],"metadata":{"id":"dbwSsF3FLLE6","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["# Encoder, seq2seq과 구조는 동일합니다.\n","class Encoder(nn.Module):\n","  def __init__(self,src_vocab_size,embedding_dim,hidden_units):\n","    super(Encoder,self).__init__()\n","    self.embedding = nn.Embedding(src_vocab_size,embedding_dim,padding_idx=0)\n","    self.lstm = nn.LSTM(embedding_dim,hidden_units,batch_first=True)\n","  def forward(self,x):\n","    # x.shape == (batchsize,seq_len,embedding_dim)\n","    x = self.embedding(x)\n","    # hidden.shape == (1,batch_size,hidden_units), cell.shape == (1,batch_size,hidden_units)\n","    outputs, (hidden,cell) = self.lstm(x)\n","    return outputs,hidden,cell"],"metadata":{"id":"HYb9lcJ3Lnyr","executionInfo":{"status":"ok","timestamp":1739463130676,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# Decoder (Luong Attention)\n","# s_t와 모든 h_i와의 dot-product를 이용하여 attention score를 구합니다.\n","# 이후, attention score를 softmax 함수를 통과시켜, attention_weights를 얻습니다.\n","# attention weights는 h_i들과 곱한 후, 더해 context vector를 얻습니다.\n","# context vector를 활용하는 방식은 다양한데 이 코드에선, embedding vector와 연결되어 input으로 활용합니다.\n","\n","class Decoder(nn.Module):\n","  def __init__(self, tar_vocab_size,embedding_dim,hidden_units):\n","    super(Decoder,self).__init__()\n","    self.embedding = nn.Embedding(tar_vocab_size,embedding_dim,padding_idx=0)\n","    self.lstm = nn.LSTM(embedding_dim+hidden_units,hidden_units,batch_first=True)\n","    self.fc = nn.Linear(hidden_units,tar_vocab_size)\n","    self.softmax = nn.Softmax(dim = 1)\n","  def forward(self, x, encoder_outputs, hidden, cell):\n","    x = self.embedding(x)\n","    # bmm = batch matix multiplication\n","    # attention scores = encoder의 모든 hidden state들과 decoder의 현재 시점 hidden state의 dot-product\n","    # scores.shape == (batch_size,source_seq_len,1)\n","    attention_scores = torch.bmm(encoder_outputs,hidden.transpose(0,1).transpose(1,2))\n","    # attention weighs = attention score의 softmax\n","    attention_weights = self.softmax(attention_scores)\n","    # context vector = encoder의 모든 hidden states와의 weighted sum\n","    context_vector = torch.bmm(attention_weights.transpose(1,2),encoder_outputs)\n","    # context vector.shape == (batch_size,1,hidden_units)\n","    # context vector isn't match second dimension of x. so, we need to extend length of context vector using repeat method for concatenating\n","    seq_len = x.shape[1]\n","    context_vector_repeated = context_vector.repeat(1,seq_len,1)\n","    # x.shape : (batch_size, target_seq_len, embedding_dim+hidden_unis)\n","    x = torch.cat((x,context_vector_repeated),dim=2)\n","\n","    # output.shape : (batch_size, target_seq_len, hidden_unis)\n","    # hidden.shape : (1, batch_size, hidden_units)\n","    # cell.shape : (1, batch_size, hidden_units)\n","    output, (hidden,cell) = self.lstm(x,(hidden,cell))\n","    # output.shape : (batch_size, target_seq_len, tar_vocab_size)\n","    output = self.fc(output)\n","\n","    return output,hidden,cell"],"metadata":{"id":"qTlMVhwnVCVp","executionInfo":{"status":"ok","timestamp":1739463130939,"user_tz":-540,"elapsed":268,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# seq2seq model을 제작합니다. encoder와 decoder를 연결하는 형태입니다.\n","# attention 자체는 seq2seq의 long sequence에서 발생하는 문제를 보정하기 위한 방법론으로 제작되었기에\n","# decoder class 내부의 코드가 attention mechanism을 정의하는 것입니다.\n","class Seq2Seq(nn.Module):\n","  def __init__(self,encoder,decoder):\n","    super(Seq2Seq,self).__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","  def forward(self,src,trg):\n","    encoder_outputs, hidden, cell = self.encoder(src)\n","    output,_,_ = self.decoder(trg,encoder_outputs, hidden, cell)\n","    return output\n","\n","encoder = Encoder(len(en_vocab),embedding_dim,hidden_units)\n","decoder = Decoder(len(ko_vocab),embedding_dim,hidden_units)\n","model = Seq2Seq(encoder,decoder)\n","\n","loss = nn.CrossEntropyLoss(ignore_index = 0)\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"WS6znpBAd5GJ","executionInfo":{"status":"ok","timestamp":1739463130939,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# evaluation function\n","def evaluation(model,dataloader,loss_function, device):\n","  model.eval()\n","  total_loss = 0.0\n","  total_correct = 0\n","  total_count = 0\n","  with torch.no_grad():\n","    for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n","      encoder_inputs = encoder_inputs.to(device)\n","      decoder_inputs = decoder_inputs.to(device)\n","      decoder_targets = decoder_targets.to(device)\n","\n","      #forward\n","      # output.shape == (batch_size, seq_len, tar_vocab_size)\n","      outputs = model(encoder_inputs, decoder_inputs)\n","\n","      # loss 계산\n","      # outputs.view(-1,outputs.size(-1)).shape == (batch_size * seq_len, tar_vocab_size)\n","      # decoder_targets.view(-1).shape == (batch_size * seq_len)\n","      total_loss += loss(outputs.view(-1,outputs.size(-1)),decoder_targets.view(-1)).item()\n","\n","      # acc 계산\n","      mask = decoder_targets != 0\n","      total_correct += ((outputs.argmax(dim=-1)==decoder_targets)*mask).sum().item()\n","      total_count += mask.sum().item()\n","\n","  return total_loss / len(dataloader), total_correct / total_count"],"metadata":{"id":"pqwUC8yFfXKO","executionInfo":{"status":"ok","timestamp":1739463130939,"user_tz":-540,"elapsed":4,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# 각 데이터를 tensor로 변환하고 batch_size 128로 설정합니다.\n","\n","encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype = torch.long)\n","decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype = torch.long)\n","decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype = torch.long)\n","\n","encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n","decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n","decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n","# 데이터셋 및 데이터로더 생성\n","batch_size = 128\n","train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor , decoder_target_test_tensor)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)  # 학습 설정\n","\n","# epoch\n","epochs = 70\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NBmeVhqhBPm","executionInfo":{"status":"ok","timestamp":1739463130939,"user_tz":-540,"elapsed":4,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"23f255f8-f0a5-40c3-d6de-1ff6f23867fa"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(2602, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5476, 256, padding_idx=0)\n","    (lstm): LSTM(512, 256, batch_first=True)\n","    (fc): Linear(in_features=256, out_features=5476, bias=True)\n","    (softmax): Softmax(dim=1)\n","  )\n",")"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["best_val_acc = float(0)\n","\n","for epoch in range(epochs):\n","  model.train()\n","  for encoder_inputs,decoder_inputs,decoder_targets in train_dataloader:\n","    encoder_inputs = encoder_inputs.to(device)\n","    decoder_inputs = decoder_inputs.to(device)\n","    decoder_targets = decoder_targets.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    # forward\n","    outputs = model(encoder_inputs,decoder_inputs)\n","\n","    # loss & backward\n","    loss_value = loss(outputs.view(-1,outputs.size(-1)),decoder_targets.view(-1))\n","    loss_value.backward()\n","\n","    # propagation\n","    optimizer.step()\n","  train_loss,train_acc = evaluation(model,train_dataloader,loss,device)\n","  val_loss,val_acc = evaluation(model,valid_dataloader,loss,device)\n","  print(f'Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {val_loss:.4f} | Valid Acc: { val_acc:.4f}')\n","\n","  if val_acc > best_val_acc :\n","    best_val_acc = val_acc\n","    print(\"best checkpoint is saved\")\n","    torch.save(model.state_dict(),'best_model.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"GDEgJ8yj8Ai5","executionInfo":{"status":"error","timestamp":1739463727346,"user_tz":-540,"elapsed":3043,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"e289d9f4-bf6d-41d6-aaac-3f3c9e82989f"},"execution_count":62,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-77d3d1ae93b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# loss & backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# best model load loss,acc 확인\n","model.load_state_dict(torch.load('best_model.pth'))\n","model.to(device)\n","\n","val_loss,val_cc = evaluation(model,valid_dataloader,loss,device)\n","\n","print(\"Best loss :\",val_loss)\n","print(\"Best acc :\",val_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P35hYK4o9bTT","executionInfo":{"status":"ok","timestamp":1739463734763,"user_tz":-540,"elapsed":956,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"93ab0f65-f8da-4b04-bd78-c14d23fd4934"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-63-824cfedb7421>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model.pth'))\n"]},{"output_type":"stream","name":"stdout","text":["Best loss : 5.9084848165512085\n","Best acc : 0.3721881390593047\n"]}]},{"cell_type":"markdown","source":["### seq2seq 기계 번역기 동작\n","- 훈련 과정에서 **교사 강요** 방식을 사용했기에 테스트 과정에서는 동작 방식이 다릅니다.\n","  - 따라서 새로 설계해줍니다.\n","- 번역 단계\n","  1. input sentence가 encoder에 입력, encoder의 마지막 시점의 hidden_state($h_t$)와 cell state를 얻습니다.\n","  2. $h_t$, cell state, \\<sos\\>를 decoder로 보냅니다.\n","  3. decoder가 \\<eos\\>를 출력할 때까지 다음 예측을 반복합니다.\n","  "],"metadata":{"id":"7u6dWB8m93k7"}},{"cell_type":"code","source":["# 정수를 단어로 변환하는 함수를 작성합니다.\n","index_to_src = {v: k for k,v in en_vocab.items()}\n","index_to_tar = {v: k for k,v in ko_vocab.items()}\n","\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if (encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if (encoded_word != 0) and (encoded_word != ko_vocab['<sos>']) and(encoded_word != ko_vocab['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence\n"],"metadata":{"id":"y57GV4cp-S9x","executionInfo":{"status":"ok","timestamp":1739463741245,"user_tz":-540,"elapsed":518,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["def decode_sequence(input_seq,model,src_vocab_size,tar_vocab_size,max_output_len, int_to_src_token, int_to_tar_token):\n","  encoder_inputs = torch.tensor(input_seq,dtype=torch.long).unsqueeze(0).to(device)\n","\n","  # set encoder initial state\n","  encoder_outputs, hidden, cell = model.encoder(encoder_inputs)\n","\n","  # <sos>를 decoder의 first input으로 설정\n","  # unsqueeze는 batch 차원을 위해 추가합니다.\n","  decoder_input = torch.tensor([3],dtype=torch.long).unsqueeze(0).to(device)\n","\n","  decoded_tokens = []\n","\n","  # for 문을 반복하며 decoder가 반복 예측하도록 합니다.\n","  for _ in range(max_output_len):\n","    output,hidden,cell = model.decoder(decoder_input,encoder_outputs,hidden,cell)\n","\n","    # 소프트맥스 회귀를 수행, 예측 단어의 인덱스\n","    output_token = output.argmax(dim=-1).item()\n","\n","    # 종료 토큰 <eos>\n","    if output_token == 4:\n","      break\n","\n","    # 각 시점의 단어(정수)는 decoded_tokens에 누적 후, 최종 번역을 진행합니다.\n","    decoded_tokens.append(output_token)\n","\n","    # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n","    decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n","  return ' '.join(int_to_tar_token[token] for token in decoded_tokens)"],"metadata":{"id":"2fk24YwN_9Nb","executionInfo":{"status":"ok","timestamp":1739463741630,"user_tz":-540,"elapsed":2,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# 테스트 결과를 확인합니다.\n","\n","for seq_index in [1123,2234,2567,1890,2353]:\n","  input_seq = encoder_input_train[seq_index]\n","  translated_text = decode_sequence(input_seq,model,len(en_vocab),len(ko_vocab),30,index_to_src,index_to_tar)\n","\n","  print(\"input sentence :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"correct sentence :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"output sentence :\",translated_text)\n","  print(\"--\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMhE5ny5B9-e","executionInfo":{"status":"ok","timestamp":1739463742840,"user_tz":-540,"elapsed":248,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"f13b5f31-f950-4df6-d255-93ec15f6812b"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["input sentence : Let the bird fly away \n","correct sentence : 새가 날아가도록 풀어주자 \n","output sentence : 그 남자 바빠 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n","----------------------------------------------------------------------------------------------------\n","input sentence : Everybody left \n","correct sentence : 모두 떠났어 \n","output sentence : 모두 떠났어 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n","----------------------------------------------------------------------------------------------------\n","input sentence : She overslept \n","correct sentence : 그 사람은 늦잠잤어 \n","output sentence : 그 사람은 늦잠잤어 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n","----------------------------------------------------------------------------------------------------\n","input sentence : I m bigger than you \n","correct sentence : 나는 너보다 크다 \n","output sentence : 나는 너보다 크다 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n","----------------------------------------------------------------------------------------------------\n","input sentence : I m sorry but I don t understand \n","correct sentence : 죄송하지만 무슨 말인지 모르겠어요 \n","output sentence : 내가 아는 한에서만 말씀드릴 수 있습니다 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n","----------------------------------------------------------------------------------------------------\n"]}]}]}