{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhlz3XdDE5QJTGHZkq16qi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 모델 만들기\n","- 순서\n","  1. 데이터 이해\n","    - 학습할 데이터를 확인하고 이해합니다.\n","    - Input, Output Data를 각각 다른 Tensor에 저장합니다.\n","  2. 가설 수립\n","    - ML에서 식을 세울 때 식을 **가설**이라고 합니다.\n","    - 가설은 임의로 추측하여 세워보는 식일 수도 있고, 경험적으로 알고 있는 식일 수도 있습니다.\n","  3. 손실 계산\n","    - cost function, loss function, error function, objective function 모두 같은 의미의 용어입니다.\n","    - 실제 값과 예측 값의 차이를 통해 모델이 잘 예측하는지 확인하기 위한 과정입니다.\n","  4. optimizer - 경사 하강법\n","    - cost function의 값을 최소로 하기 위해 사용되는 알고리즘 ( optimizer 알고리즘 )\n","    - optimizer는 cost function의 결과를 W로 미분하여 구한 기울기에 특정 숫자(학습률)를 곱한 값을 빼서 새로운 W로 사용하는 식으로 이용됩니다.\n","  - 문제에 따라 적절한 hypothesis, cost function, optimizer는 모두 다릅니다.\n","  \n"],"metadata":{"id":"BmfNISEKLce8"}},{"cell_type":"markdown","source":["# Implementing Linear Regression using Pytorch\n","\n","- setting (import)\n","- variable allocation\n","- initialize weight and bias\n","- establish hypothesis\n","- declare cost function\n","- implementing gradient descent\n","- full code"],"metadata":{"id":"paSwY1L5O8bj"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kr-wlSioKuBn","executionInfo":{"status":"ok","timestamp":1739331771538,"user_tz":-540,"elapsed":2,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"c51e6b8b-02f3-4d30-8d6a-83cae8b06729"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7da97cb8aa90>"]},"metadata":{},"execution_count":2}],"source":["# setting import\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","\n","torch.manual_seed(42) # for reproducibility of model"]},{"cell_type":"code","source":["# variable allocation\n","x_train = torch.FloatTensor(np.arange(10))\n","y_train = torch.FloatTensor(np.arange(0,30,3))\n","print(x_train) #값 확인\n","print(x_train.shape) #size확인\n","print(y_train)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-EvJVjYQELd","executionInfo":{"status":"ok","timestamp":1739331843432,"user_tz":-540,"elapsed":245,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"3f9c7fc5-8c12-4a8c-fa30-fc224000d9bf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n","torch.Size([10])\n","tensor([ 0.,  3.,  6.,  9., 12., 15., 18., 21., 24., 27.])\n","torch.Size([10])\n"]}]},{"cell_type":"code","source":["# initial weight and bias\n","W = torch.zeros(1,requires_grad=True) # requires_grad를 True함으로서 torch가 이 tensor에 대한 연산을 추적하게 합니다.\n","b = torch.zeros(1, requires_grad=True)\n","print(\"W =\",W)\n","print(\"b =\",b)\n","# 현재 식 y = 0x+0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jya2sZTVQcOT","executionInfo":{"status":"ok","timestamp":1739332541388,"user_tz":-540,"elapsed":261,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"4a6a351a-e739-4bf7-948c-e3425eec1217"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["W = tensor([0.], requires_grad=True)\n","b = tensor([0.], requires_grad=True)\n"]}]},{"cell_type":"code","source":["# establish hypothesis\n","hypo = x_train*W+b # linear regression 이므로 y = Wx+b 형태\n","print(hypo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyxwYyXFTGZp","executionInfo":{"status":"ok","timestamp":1739332604246,"user_tz":-540,"elapsed":1,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"99803b56-57c9-4f8a-d2f2-331306e0d3e5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["# declare cost function\n","cost = torch.mean((hypo-y_train)**2) # MSE 구현\n","print(cost)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAZajnfyTWka","executionInfo":{"status":"ok","timestamp":1739332665917,"user_tz":-540,"elapsed":312,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"30c1a187-8953-443f-fcb5-85ed294c771d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(256.5000, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["# implementing gradient descent\n","\n","optimizer = optim.SGD([W,b],lr = 0.01) # pytorch optim SGD를 사용합니다. 이때 학습 대상(W,b)를 인자로 전달해주고, learning rate는 0.01로 설정합니다."],"metadata":{"id":"Lf4WajGLTks3","executionInfo":{"status":"ok","timestamp":1739332949450,"user_tz":-540,"elapsed":7135,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# how to using optimizer\n","optimizer.zero_grad # gradient initialize\n","cost.backward() #W,b 에 대한 gradinet 계산\n","optimizer.step() # W,b update"],"metadata":{"id":"QvCKcdO0T9sG","executionInfo":{"status":"ok","timestamp":1739332949450,"user_tz":-540,"elapsed":1,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["X_train = torch.FloatTensor([1,2,3])\n","Y_train = torch.FloatTensor([4,5,6])\n","\n","W = torch.zeros(1,requires_grad=True)\n","b = torch.zeros(1,requires_grad=True)\n","\n","optimizer = optim.SGD([W,b],lr=0.01)\n","\n","epochs = 1000\n","for epoch in range(1,epochs+1):\n","  output = X_train*W + b\n","  loss = torch.mean((Y_train-output)**2)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  if epoch%100 == 0:\n","    print(f\"epoch {epoch}/{epochs} \\| W : {W} b : {b} grad : {W.grad}\") # grad가 점점 줄어드는 것을 확인할 수 있습니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMVegb7xUo0i","executionInfo":{"status":"ok","timestamp":1739338388761,"user_tz":-540,"elapsed":651,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"6a13b23d-01d4-4bbb-8be4-b29deab31c0c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 100/1000 \\| W : tensor([1.7417], requires_grad=True) b : tensor([1.3138], requires_grad=True) grad : tensor([0.1786])\n","epoch 200/1000 \\| W : tensor([1.5831], requires_grad=True) b : tensor([1.6745], requires_grad=True) grad : tensor([0.1405])\n","epoch 300/1000 \\| W : tensor([1.4584], requires_grad=True) b : tensor([1.9580], requires_grad=True) grad : tensor([0.1105])\n","epoch 400/1000 \\| W : tensor([1.3603], requires_grad=True) b : tensor([2.1809], requires_grad=True) grad : tensor([0.0868])\n","epoch 500/1000 \\| W : tensor([1.2832], requires_grad=True) b : tensor([2.3561], requires_grad=True) grad : tensor([0.0682])\n","epoch 600/1000 \\| W : tensor([1.2226], requires_grad=True) b : tensor([2.4939], requires_grad=True) grad : tensor([0.0537])\n","epoch 700/1000 \\| W : tensor([1.1750], requires_grad=True) b : tensor([2.6021], requires_grad=True) grad : tensor([0.0422])\n","epoch 800/1000 \\| W : tensor([1.1376], requires_grad=True) b : tensor([2.6872], requires_grad=True) grad : tensor([0.0332])\n","epoch 900/1000 \\| W : tensor([1.1082], requires_grad=True) b : tensor([2.7541], requires_grad=True) grad : tensor([0.0261])\n","epoch 1000/1000 \\| W : tensor([1.0850], requires_grad=True) b : tensor([2.8067], requires_grad=True) grad : tensor([0.0205])\n"]}]},{"cell_type":"code","source":["# zero_grad를 하지 않는다면?\n","X_train = torch.FloatTensor([1,2,3])\n","Y_train = torch.FloatTensor([4,5,6])\n","\n","W = torch.zeros(1,requires_grad=True)\n","b = torch.zeros(1,requires_grad=True)\n","\n","optimizer = optim.SGD([W,b],lr=0.01)\n","\n","epochs = 1000\n","for epoch in range(1,epochs+1):\n","  output = X_train*W + b\n","  loss = torch.mean((Y_train-output)**2)\n","  #optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  if epoch%100 == 0:\n","    print(f\"epoch {epoch}/{epochs} \\| W : {W} b : {b} grad : {W.grad}\") # grad가 점점 줄어드는 것을 확인할 수 있습니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2g-MXoI7mlN8","executionInfo":{"status":"ok","timestamp":1739338401008,"user_tz":-540,"elapsed":542,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"abc23d9e-088f-4573-9d91-faa2ba5b8208"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 100/1000 \\| W : tensor([2.3831], requires_grad=True) b : tensor([3.0605], requires_grad=True) grad : tensor([-62.9186])\n","epoch 200/1000 \\| W : tensor([0.9958], requires_grad=True) b : tensor([5.3501], requires_grad=True) grad : tensor([51.6932])\n","epoch 300/1000 \\| W : tensor([-1.5019], requires_grad=True) b : tensor([3.3407], requires_grad=True) grad : tensor([13.3790])\n","epoch 400/1000 \\| W : tensor([2.6057], requires_grad=True) b : tensor([1.9019], requires_grad=True) grad : tensor([-58.9015])\n","epoch 500/1000 \\| W : tensor([2.8937], requires_grad=True) b : tensor([1.7063], requires_grad=True) grad : tensor([44.6695])\n","epoch 600/1000 \\| W : tensor([-1.3236], requires_grad=True) b : tensor([2.9744], requires_grad=True) grad : tensor([14.5651])\n","epoch 700/1000 \\| W : tensor([0.7569], requires_grad=True) b : tensor([5.3984], requires_grad=True) grad : tensor([-63.4597])\n","epoch 800/1000 \\| W : tensor([2.3503], requires_grad=True) b : tensor([3.5491], requires_grad=True) grad : tensor([44.8915])\n","epoch 900/1000 \\| W : tensor([0.0289], requires_grad=True) b : tensor([0.0504], requires_grad=True) grad : tensor([28.3669])\n","epoch 1000/1000 \\| W : tensor([1.7423], requires_grad=True) b : tensor([2.4133], requires_grad=True) grad : tensor([-69.9603])\n"]}]}]}