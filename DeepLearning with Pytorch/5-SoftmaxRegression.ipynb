{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["gDNG26R52tQA"],"authorship_tag":"ABX9TyNvFvpkhhGhLSDQEJ18DzhP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 원-핫 인코딩 (One-Hot Encoding)\n","- One-Hot Encoding은 선택지의 개수만큼 Index를 가지며, 해당 되는 원소에는 1, 그 외에는 0으로 표시하는 Encoding 방식입니다.\n","  - EX) 수박,사과,딸기 일때, 딸기는 One-Hot Encoding으로 [0,0,1]로 표현합니다.\n","- One-Hot Encoding으로 표현한 벡터를 **One-Hot Vector**라고 합니다.\n","\n","## One-Hot Encoding 사용하는 이유\n","- Class 별로 동일한 가중치를 갖기 때문에 One-Hot Vector가 적절합니다.\n","  - 만약, 1,2,3,4로 표현을 한다고 가정한다면 MSE를 구할 때, 4를 1로 잘못 예측한 것과 2를 1로 잘못 예측한 것이 오차 수준이 다릅니다.\n","  - 만약 이러한 방식으로 그냥 사용을 한다면, 4를 1로 예측한 것이 오차가 크기 때문에 정답(1)에 에러 작은 오답(2)가 더 가깝고, 에러가 큰 오답(4)는 관계가 더 적다고 모델이 인식합니다.\n","    - 하지만, 실제에서는 1이 아니라면 2나 4 모두 동일하게 잘못된 것이므로 이러한 인식은 옳지 않습니다.\n","  - 이러한 이유로 One-Hot Encoding이 적절합니다.\n","- 하지만, One-Hot Encoding은 이러한 장점 때문에 유사성을 알 수 없다는 단점도 존재합니다.\n","  - 클래스간의 관계를 알 수 없기 때문입니다."],"metadata":{"id":"yb9TF_f4KboS"}},{"cell_type":"markdown","source":["## Class가 3개 이상인 상황의 분류 - SoftmaxRegression\n","1. Multi-class Classification\n","  - Softmax를 사용합니다.\n","    - Binary Classification에서도 두 선택지의 확률의 합이 1이었습니다. 이것을 확장하여 n개의 선택지가 된다면, n개 선택지의 확률의 합이 1인 함수를 사용하면 됩니다.\n","  - Softmax Function에서 $p_i$\n","    - $p_i = \\frac{e^{z_i}}{Σ_{j=1}^{k}e^{z_j}}$ for i = 1,2,...,k\n","  - 실제 구현\n","    - Feature 4개로 3개의 클래스를 분류해야 하는 상황이라고 가정합니다.\n","      1. 3개의 클래스로 분류하므로 softmax는 3개의 확률값을 출력합니다.\n","      2. feature가 4개이고, class가 3개이므로 가중치는 12개가 필요합니다.\n","        - 따라서 W.shape == (3,4), x.shape == (4,1), b.shape == (3,1)입니다.\n","\n","2. Softmax Regression에서 Loss Function\n","  - Cross Entropy Loss\n","    - 1개의 값에 대한 Loss Function\n","      - $Loss(W) = -Σ_{j=1}^{k}y_jlog(p_j)$\n","    - 전체에 대한 Loss Function\n","      - $Loss(W) = -\\frac{1}{n}Σ_{i=1}^{n}Σ_{j=1}^{k}y_jlog(p_j)$\n","  - Softmax에서 사용한 CrossEntropyLoss도 BinaryClassification에서 이용한 CrossEntropyLoss와 본질적으로 동일합니다."],"metadata":{"id":"kq3b0YufLwV-"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"_B60cmDfKFAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739944507979,"user_tz":-540,"elapsed":2,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"91d8d55d-c4fc-46f5-8230-56615c62d964"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7e6f2cd50a70>"]},"metadata":{},"execution_count":4}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)"]},{"cell_type":"code","source":["# soft max test\n","z = torch.FloatTensor([1,2,3])\n","H = F.softmax(z,dim = 0)\n","print(H) # softmax결과의 합이 1임을 알 수 있습니다.\n","print(H.sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hmrAVwOxkHM","executionInfo":{"status":"ok","timestamp":1739944519846,"user_tz":-540,"elapsed":9,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"1504ebc1-2005-4343-ffce-f74d35e0fd9e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0900, 0.2447, 0.6652])\n","tensor(1.)\n"]}]},{"cell_type":"code","source":["# softmax test\n","z = torch.rand(3,5,requires_grad=True)\n","\n","H = F.softmax(z,dim=1)\n","# 각 행별로 합이 1임을 알 수 있습니다. (5개의 클래스를 분류하는 것입니다.)\n","print(H)\n","print(H.sum(dim=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-ZklaLJxWEm","executionInfo":{"status":"ok","timestamp":1739944654904,"user_tz":-540,"elapsed":11,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"0b35e34b-2725-4051-e6b0-95b806f114d2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2441, 0.1429, 0.2298, 0.2344, 0.1487],\n","        [0.1665, 0.2504, 0.2309, 0.1707, 0.1815],\n","        [0.2733, 0.1576, 0.2292, 0.2147, 0.1252]], grad_fn=<SoftmaxBackward0>)\n","tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"]}]},{"cell_type":"code","source":["# 직접구현 시작\n","\n","# y 는 정답 레이블을 나타냅니다.\n","y = torch.randint(5,(3,)).long()\n","print(y) # y 는 0~4 중의 한 개의 값을 갖는 원소들을 (3,)형태로 갖는 텐서입니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rh1zAkoFx9uC","executionInfo":{"status":"ok","timestamp":1739944782693,"user_tz":-540,"elapsed":47,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"a2f3d28f-6443-43a0-d8e4-04218f4f1fe5"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2, 1, 4])\n"]}]},{"cell_type":"code","source":["# y(정답레이블)을 one-hot vector로 표현합니다.\n","#각 레이블에 대해 One-Hot Encoding을 진행합니다.\n","y_one_hot = torch.zeros_like(z) # z의 크기인 3 x 5 크기인 0 텐서를 생성합니다.\n","y_one_hot.scatter_(1,y.unsqueeze(1),1)\n","\"\"\"\n","scatter\n","first param - 값을 scatter할 차원을 표현 (각 열이 클래스를 의미하므로 1번째 차원(열)에 따라 뿌리게 설정)\n","second param - 값을 뿌릴 위치 표현 (y는 위에서처럼 정답 class를 의미하므로 해당 정답 클래스의 위치에 1을 뿌립니다.)\n","third param - 뿌릴 값을 표현 (one-hot vector를 만들어야 하므로 1을 뿌립니다.)\n","\"\"\"\n","y_one_hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJ1LB7tPynVj","executionInfo":{"status":"ok","timestamp":1739945172144,"user_tz":-540,"elapsed":44,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"2c54a0bb-c7be-40e6-8555-abfabe696a22"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 1., 0., 0.],\n","        [0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 1.]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Loss Function을 구현합니다.\n","loss = (y_one_hot * -torch.log(H)).sum(dim=1).mean()\n","\"\"\"\n","y_one_hot * -torch.log(H).sum(dim=1) 으로 각 클래스의 오차를 합합니다.\n",".mean()으로 앞에서 구한 오차들의 평균을 구합니다.\n","\"\"\"\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8B_Ev9X70UPX","executionInfo":{"status":"ok","timestamp":1739945618602,"user_tz":-540,"elapsed":10,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"dd920aec-2d9f-484e-cf69-472055d15b30"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.6445, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## High-level Loss Implementation"],"metadata":{"id":"Vguiewpe6-SJ"}},{"cell_type":"markdown","source":["### nll_loss\n","- Negative Log Likelihood를 계산하는 함수 (multi classification에서 주로 사용)\n","- nll_loss Input\n","  - first param : 모델의 예측값에 log를 취한 값, (F.log_softmax()는 softmax(예측)을 한 뒤, log를 취했으므로 적합)\n","  - second param : 실제 정답 클래스를 나타내는 텐서, (y 가 정답 레이블이었으므로 적합)\n","    - y_one_hot이 아닌, y를 사용하는 이유\n","      - nll_loss가 내부적으로 class index를 사용하여 loss를 계산하도록 설계되었기 때문입니다.\n","- nll_loss는 이 두가지 input바탕으로 negative log likelihood를 계산합니다.\n","  - 즉, model이 예측한 probability distribution과 실제 정답 label 사이의 차이를 측정합니다.\n","  "],"metadata":{"id":"gDNG26R52tQA"}},{"cell_type":"code","source":["# Loss Function High-level implementation\n","## H = softmax이므로, -torch.log(softmax())가 됩니다. 이것은 -F.log_softmax()로 변환할 수 있습니다.\n","## 또한 nll_loss (negative log likelihood)를 이용하면, -F.log_softmax()이 외의 연산도 대체가 가능하므로 아래와 같이 표현할 수 있습니다.\n","loss = F.nll_loss(F.log_softmax(z,dim=1),y)\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ll3_tvGY1wYA","executionInfo":{"status":"ok","timestamp":1739946955377,"user_tz":-540,"elapsed":2,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"81f4f4dc-f593-41ee-b489-1a8892495de7"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.6445, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### cross_entropy\n","- Cross Entropy Loss를 계산하는 함수 (multi classification에서 주로 사용)\n","- cross_entropy input\n","  - first param : model's original output (original output before applying softmax like **z**)\n","  - second param : label ( it presents real class of each data like **y** )\n","- cross_entropy's detail\n","  - loss computation\n","    1. LogSoftmax : Compute log probability by applying log_softmax function on model's original output\n","    2. NLLLoss : Compute negative log likelihood by applying nll_loss function using log probability and label"],"metadata":{"id":"1G4jUO9B5KdQ"}},{"cell_type":"code","source":["loss = F.cross_entropy(z,y)\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZgnWH0TS6wOd","executionInfo":{"status":"ok","timestamp":1739946956899,"user_tz":-540,"elapsed":7,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"1a3e4d9f-7794-4865-f1c7-56e4f155a2d7"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.6445, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Softmax Regression Implementation\n"],"metadata":{"id":"L28h_IJ-7CUf"}},{"cell_type":"code","source":["# import\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1FILFPh9QVo","executionInfo":{"status":"ok","timestamp":1739947607718,"user_tz":-540,"elapsed":8,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"e42bcfc9-d83e-41c0-ccfa-beef55c75d14"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7e6f2cd50a70>"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["#data\n","## feature : 4 , sample : 8, class : 3\n","x_train = [[1, 2, 1, 1],\n","            [2, 1, 3, 2],\n","            [3, 1, 3, 4],\n","            [4, 1, 5, 5],\n","            [1, 7, 5, 5],\n","            [1, 2, 5, 6],\n","            [1, 6, 6, 6],\n","            [1, 7, 7, 7]]\n","y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n","x_train = torch.FloatTensor(x_train)\n","y_train = torch.LongTensor(y_train)"],"metadata":{"id":"l1Bw5pk09Yxl","executionInfo":{"status":"ok","timestamp":1739947625023,"user_tz":-540,"elapsed":1,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["### low-level (directly implementation)"],"metadata":{"id":"WXtL8zCN98v7"}},{"cell_type":"code","source":["y_one_hot = torch.zeros((8,3))\n","y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n","y_one_hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EbJFrf1R91HS","executionInfo":{"status":"ok","timestamp":1739947821900,"user_tz":-540,"elapsed":3,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"2498cd9d-0874-47b1-a379-be465e59f729"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 1.],\n","        [0., 0., 1.],\n","        [0., 0., 1.],\n","        [0., 1., 0.],\n","        [0., 1., 0.],\n","        [0., 1., 0.],\n","        [1., 0., 0.],\n","        [1., 0., 0.]])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["W = torch.zeros((4,1),requires_grad=True)\n","b = torch.zeros((1,3),requires_grad=True)\n","opt = optim.SGD([W,b],lr = 0.1)"],"metadata":{"id":"yRRjQ4AO-NVe","executionInfo":{"status":"ok","timestamp":1739947900237,"user_tz":-540,"elapsed":7509,"user":{"displayName":"전길원","userId":"12549511235485585617"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["epochs = 1000\n","for epoch in range(epochs):\n","  output = F.softmax(x_train.matmul(W)+b,dim=1)\n","  loss = (y_one_hot * -torch.log(output)).sum(dim=1).mean()\n","\n","  opt.zero_grad()\n","  loss.backward()\n","  opt.step()\n","\n","  if epoch % 100 == 0 :\n","    print('Epoch {:4d}/{} loss: {:.6f}'.format(epoch, epochs, loss.item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PX4j6TnK-ehq","executionInfo":{"status":"ok","timestamp":1739948076866,"user_tz":-540,"elapsed":669,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"8c9d5062-471a-4b6e-b28a-851ce29b0dbf"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 loss: 1.098612\n","Epoch  100/1000 loss: 1.082238\n","Epoch  200/1000 loss: 1.082196\n","Epoch  300/1000 loss: 1.082196\n","Epoch  400/1000 loss: 1.082196\n","Epoch  500/1000 loss: 1.082196\n","Epoch  600/1000 loss: 1.082196\n","Epoch  700/1000 loss: 1.082196\n","Epoch  800/1000 loss: 1.082195\n","Epoch  900/1000 loss: 1.082195\n"]}]},{"cell_type":"markdown","source":["## High-levle"],"metadata":{"id":"t6boe1T__r4F"}},{"cell_type":"code","source":[],"metadata":{"id":"maFkjvT3_Llc"},"execution_count":null,"outputs":[]}]}