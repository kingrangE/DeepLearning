{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPywWL8jLZO39Q3IPEAKySi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 파이토치 패키지의 기본 구성\n","1. `torch`\n","  - main namespace\n","  - 포함하는 항목\n","    - tensor 등의 수학 함수\n","  - numpy와 유사한 구조를 가집니다.\n","2. `torch.autograd`\n","  - 포함하는 항목\n","    - auto differential을 위한 함수\n","    - auto differential on/off를 제어하는 context_manager (enable_grad,no_grad)\n","    - 자체 미분 가능 함수를 정의할 때 사용하는 base class `Function`이 포함되어 있습니다.\n","3. `torch.nn`\n","  - 포함하는 항목\n","    - neural network를 구축하기 위한 다양한 Data Structure, Layer 정의\n","      - ex) RNN,LSTM,ReLU,MSELoss 등\n","4. `torch.optim`\n","  - SGD(Stochastic Gradient Descent)를 중심으로 한 파라미터 최적화 알고리즘을 가집니다.\n","5. `torch.utils.data`\n","  - SGD의 반복 연산을 실행할 때 사용하는 mini batch용 utility function이 정의되어 있습니다.\n","6. `torch.onnx`\n","  - ONNX format으로 model을 export할 때 사용합니다.\n","    - ONNX는 서로 다른 딥러닝 프레임워크 간에 모델을 공유할 때 사용하는 format\n","\n","### Pytorch Model을 훈련하기 위해서는 데이터를 Tensor의 형태로 만들어야 합니다."],"metadata":{"id":"quA9Dsf1ky8N"}},{"cell_type":"markdown","source":["## Tesnor Manipulation\n","### Index\n","- 벡터, 행렬, 그리고 텐서\n","- Numpy Review\n","- Pytorch Tensor Allocation\n","- Matrix Multiplication\n","- Other Basic Ops"],"metadata":{"id":"vokTDNgJmWNf"}},{"cell_type":"markdown","source":["### Vector, Matrix, Tensor\n","- 3개가 딥러닝의 **기본 단위**입니다.\n","- vector ( 1-d Tensor )\n","  - 1차원으로 구성된 값\n","- Matrix ( 2-d Tensor )\n","  - 2차원으로 구성된 값\n","- Tensor ( 3-d Tensor )\n","  - 3차원으로 구성된 값"],"metadata":{"id":"FurpalH8nA4P"}},{"cell_type":"markdown","source":["### Pytorch Tensor Allocation 실습\n","1. 1D with PyTorch\n","2. 2D with Pytorch\n","3. Broadcasting\n","4. Matrix Multiplication Vs Multiplication\n","5. Mean\n","6. Sum\n","7. Max, Argmax\n","8. View\n","9. Squeeze\n","10. Unsqueeze\n","11. Type Casting\n","12. concatenate\n","13. Stacking\n","14. ones_like, zeros_lie\n","15. Inplace Operation\n"],"metadata":{"id":"-QTpu0BHsYSC"}},{"cell_type":"code","source":["# 1-D Tensor\n","import torch\n","t = torch.Tensor([0.,1.,2.,3.,4.])\n","print(t)\n","print(t.ndim) # dimension\n","print(t.shape) # shape\n","print(t.size()) # shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIvQPwAJt6Ao","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":11124,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"a8bfb4dc-f057-4f78-99d0-5d91ec1de09a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 1., 2., 3., 4.])\n","1\n","torch.Size([5])\n","torch.Size([5])\n"]}]},{"cell_type":"code","source":["# 2-D Tensor\n","t2 = torch.Tensor([[1.,2.,3.],\n","                   [4.,5.,6.],\n","                   [7.,8.,9.]])\n","print(t2)\n","print(t2.ndim)\n","print(t2.shape)\n","print(t2.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiYGUVs4vVWu","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"c581284c-69c8-46aa-b18e-0bd9af0a5417"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","2\n","torch.Size([3, 3])\n","torch.Size([3, 3])\n"]}]},{"cell_type":"code","source":["# BroadCasting\n","## 행렬의 크기가 다른 경우 자동으로 크기를 맞춰 수행하는 기능\n","m1 = torch.Tensor([1,2,3])\n","m2 = torch.Tensor([4])\n","m3 = torch.Tensor([5,6])\n","print(m1+m2)\n","#print(m1+m3) #모든 경우에 되는 것은 X (둘이 약수 관계여야 합니다.)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6x4SXDlYwC7B","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":5,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"9cdbb592-fcb5-45bb-fb4e-9b9802888730"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([5., 6., 7.])\n"]}]},{"cell_type":"code","source":["# Matrix Multiplication Vs Multiplication\n","## matmul -> 내적(행렬곱셈) , multiplication -> 원소별 곱셈\n","m1 = torch.Tensor([[2,3],[4,5]])\n","m2 = torch.Tensor([10,20])\n","print(torch.matmul(m1,m2)) # m1 내적 m2\n","print(m1.matmul(m2)) # m1 내적 m2\n","print(m2.matmul(m1)) # m2 내적 m1\n","# 행렬 곱은 순서가 결과에 영향을 미칩니다.\n","## multiplication -> 원소별 곱셈\n","print(torch.multiply(m1,m2)) # m1 * m2\n","print(m1.mul(m2)) # m1 * m2\n","print(m2.mul(m1)) # m2 * m1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07sHmeFkwdDg","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":4,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"6891c8f9-f5a2-4b01-bffa-e513324547f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 80., 140.])\n","tensor([ 80., 140.])\n","tensor([100., 130.])\n","tensor([[ 20.,  60.],\n","        [ 40., 100.]])\n","tensor([[ 20.,  60.],\n","        [ 40., 100.]])\n","tensor([[ 20.,  60.],\n","        [ 40., 100.]])\n"]}]},{"cell_type":"code","source":["# Mean -> 모든 원소 합의 평균\n","\n","t = torch.Tensor([1,2])\n","print(t.mean()) # 1-D tensor의 mean\n","t2 = torch.Tensor([[1,2],[4,5]])\n","print(t2.mean()) # 2-D tensor의 mean\n","\n","## 열의 평균을 구하는 방법\n","print(\"열의 평균\")\n","print(t2.mean(dim=0)) # dim = 0 을 제거한다는 의미 (dim=0 (행))\n","## 행의 평균\n","print(\"행의 평균\")\n","print(t2.mean(dim=1)) # dim = 1 을 제거한다는 의미 (dim=1 (열))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBeVQt6awc-k","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":4,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"84ca1197-8ffe-42f8-cd96-4d3757ea47df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.5000)\n","tensor(3.)\n","열의 평균\n","tensor([2.5000, 3.5000])\n","행의 평균\n","tensor([1.5000, 4.5000])\n"]}]},{"cell_type":"code","source":["# Sum -> 원소의 합\n","print(\"sum of 1-d array\")\n","print(t.sum())\n","print(\"++++++++++++++++\")\n","print(\"sum of 2-d array\")\n","print(t2.sum())\n","print(\"+++++++++++++++\")\n","print(\"sum of column of 2-d array\")\n","print(t2.sum(dim=0))\n","print(\"+++++++++++++++\")\n","print(\"sum of row of 2-d array\")\n","print(t2.sum(dim=1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0uNetukx4Yi","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":3,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"6154f723-3060-4c88-9619-bb4e5d0dd8d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sum of 1-d array\n","tensor(3.)\n","++++++++++++++++\n","sum of 2-d array\n","tensor(12.)\n","+++++++++++++++\n","sum of column of 2-d array\n","tensor([5., 7.])\n","+++++++++++++++\n","sum of row of 2-d array\n","tensor([3., 9.])\n"]}]},{"cell_type":"code","source":["# Max, ArgMax\n","## Max -> 원소의 최대값\n","## Argmax -> 최대값을 가진 Index\n","print(\"t2의 Max\")\n","print(t2.max())\n","print(\"++++++++++\")\n","print(\"t2의 Argmax\")\n","print(t2.argmax())\n","## column,row별 최대값 (dim인자를 이용하게 되면 indice로 argmax까지 자동으로 알려줍니다.)\n","print(\"Max value per column of t2\")\n","print(t2.max(dim = 0))\n","print(\"Max value per row of t2\")\n","print(t2.max(dim = 1))\n","print(\"Argmax per column of t2\")\n","print(t2.argmax(dim = 0))\n","print(\"Argmax per row of t2\")\n","print(t2.argmax(dim = 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rsxqw-Tdyy5q","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":3,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"39650b10-0cc7-4b2a-8611-c7a508bc247f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["t2의 Max\n","tensor(5.)\n","++++++++++\n","t2의 Argmax\n","tensor(3)\n","Max value per column of t2\n","torch.return_types.max(\n","values=tensor([4., 5.]),\n","indices=tensor([1, 1]))\n","Max value per row of t2\n","torch.return_types.max(\n","values=tensor([2., 5.]),\n","indices=tensor([1, 1]))\n","Argmax per column of t2\n","tensor([1, 1])\n","Argmax per row of t2\n","tensor([1, 1])\n"]}]},{"cell_type":"code","source":["# View\n","## numpy의 reshape역할\n","import numpy as np\n","t = torch.Tensor(np.arange(16)) # 16,1 크기의 tensor 생성\n","## 현재 Tensor size 확인\n","print(\"변경 전 Tensor size\")\n","print(t.size())\n","print(\"+++++++++++++\")\n","## view를 이용하여 4,2,2로 변경\n","t_ver2 = t.view(4,2,2)\n","## 변경 뒤 Tensor size 확인\n","print(\"3차원 변경 후 Tensor size\")\n","print(t_ver2.size())\n","print(\"+++++++++++++\")\n","## view -1 기능 이용\n","### view에서 -1은 사용자가 설정하지 않고 남은 몫을 넣도록 하는 의미입니다. (ex, 6x3 을 view(-1,6)이라고 하면 18(3x6)을 6으로 나눈 몫 3이 -1로 들어갑니다. )\n","t_ver3 = t.view(-1,4)\n","print(\"-1을 이용하여 2차원으로 변경 후 Tensor size\")\n","print(t_ver3.size())\n","print(\"++++++++++++++\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iegJjyoazz2n","executionInfo":{"status":"ok","timestamp":1739328148486,"user_tz":-540,"elapsed":2,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"c49322a6-4643-4fb9-a6f5-16070e848b17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["변경 전 Tensor size\n","torch.Size([16])\n","+++++++++++++\n","3차원 변경 후 Tensor size\n","torch.Size([4, 2, 2])\n","+++++++++++++\n","-1을 이용하여 2차원으로 변경 후 Tensor size\n","torch.Size([4, 4])\n","++++++++++++++\n"]}]},{"cell_type":"code","source":["# Squeeze\n","## 차원이 1인 경우 1인 차원을 제거하는 기능\n","t = torch.Tensor([[1],[2],[3]])\n","print(\"Tensor t\")\n","print(t)\n","print(\"+++++++++++++++\")\n","print(\"t size\")\n","print(t.size()) # size 확인\n","print(\"+++++++++++++++\")\n","print(\"squeeze\")\n","print(t.squeeze()) # 1인 차원을 제거\n","print(\"+++++++++++++++\")\n","print(\"1인 차원 제거 후의 t shape\")\n","print(t.squeeze().shape) # 1이었던 두 번째 차원이 제거"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xxwHCcH2ifz","executionInfo":{"status":"ok","timestamp":1739328149433,"user_tz":-540,"elapsed":948,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"98b816bd-ca66-4277-d64e-46d6cd069148"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor t\n","tensor([[1.],\n","        [2.],\n","        [3.]])\n","+++++++++++++++\n","t size\n","torch.Size([3, 1])\n","+++++++++++++++\n","squeeze\n","tensor([1., 2., 3.])\n","+++++++++++++++\n","1인 차원 제거 후의 t shape\n","torch.Size([3])\n"]}]},{"cell_type":"code","source":["# Unsqueeze\n","t = torch.Tensor([[1],[2],[3]])\n","print(\"Tensor t\")\n","print(t)\n","print(\"+++++++++++++++\")\n","print(\"t size\")\n","print(t.size()) # size 확인\n","print(\"+++++++++++++++\")\n","print(\"unsqueeze\")\n","print(t.unsqueeze(0)) # 1인 차원을 제거\n","print(\"+++++++++++++++\")\n","print(\"맨 앞에 새로운 차원 추가 후의 t shape\")\n","print(t.unsqueeze(0).shape) # 1이었던 두 번째 차원이 제거"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMtqDRyW3JMq","executionInfo":{"status":"ok","timestamp":1739328149433,"user_tz":-540,"elapsed":3,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"73065182-b1f7-4f19-c1e5-6907e0fda0d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor t\n","tensor([[1.],\n","        [2.],\n","        [3.]])\n","+++++++++++++++\n","t size\n","torch.Size([3, 1])\n","+++++++++++++++\n","unsqueeze\n","tensor([[[1.],\n","         [2.],\n","         [3.]]])\n","+++++++++++++++\n","맨 앞에 새로운 차원 추가 후의 t shape\n","torch.Size([1, 3, 1])\n"]}]},{"cell_type":"code","source":["# TypeCasting\n","## 자료형을 변환하는 기능\n","### long type tensor 선언\n","lt = torch.LongTensor([1,2,3,4])\n","print(\"long tensor\")\n","print(lt.dtype)\n","print(\"++++++++++++\")\n","### long -> float 변경\n","ft = lt.float()\n","print(\"float tensor\")\n","print(ft.dtype)\n","print(\"++++++++++++\")\n","### Byte Type tensor 선언\n","bt = torch.ByteTensor([True,False,True,False])\n","print(\"byte tensor\")\n","print(bt.dtype)\n","print(\"++++++++++++\")\n","### byte -> long, float 변경\n","bt_to_lt = bt.long()\n","bt_to_ft = bt.float()\n","print(\"bt -> lt\")\n","print(bt_to_lt.dtype)\n","print(\"++++++++++++\")\n","print(\"bt -> ft\")\n","print(bt_to_ft.dtype)\n","print(\"++++++++++++\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctYJIAQc3tt5","executionInfo":{"status":"ok","timestamp":1739328149433,"user_tz":-540,"elapsed":3,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"6c491bfa-fde6-4285-9d8b-a7067a61908b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["long tensor\n","torch.int64\n","++++++++++++\n","float tensor\n","torch.float32\n","++++++++++++\n","byte tensor\n","torch.uint8\n","++++++++++++\n","bt -> lt\n","torch.int64\n","++++++++++++\n","bt -> ft\n","torch.float32\n","++++++++++++\n"]}]},{"cell_type":"code","source":["# Concatenate\n","## Tensor를 연결하는 것\n","x1 = torch.Tensor([[1,2],[3,4]])\n","x2 = torch.Tensor([[5,6],[7,8]])\n","## Tensor를 첫 번째 차원으로 연결\n","print(\"첫 번째 차원(행)으로 연결 (행의 개수 증가)\")\n","concat_dim1= torch.cat([x1,x2],dim=0)\n","print(concat_dim1)\n","## Tensor를 두 번째 차원으로 연결\n","print(\"두 번째 차원(열)으로 연결 (열의 개수 증가)\")\n","concat_dim2= torch.cat([x1,x2],dim=1)\n","print(concat_dim2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmsgT7QD5OGE","executionInfo":{"status":"ok","timestamp":1739328149433,"user_tz":-540,"elapsed":2,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"02e17c78-4c41-4415-98da-8557a2eb38c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["첫 번째 차원(행)으로 연결 (행의 개수 증가)\n","tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.],\n","        [7., 8.]])\n","두 번째 차원(열)으로 연결 (열의 개수 증가)\n","tensor([[1., 2., 5., 6.],\n","        [3., 4., 7., 8.]])\n"]}]},{"cell_type":"code","source":["# Stacking\n","## concat과 다른 연결 방식\n","x = torch.FloatTensor([1,4])\n","y = torch.FloatTensor([2,5])\n","z = torch.FloatTensor([3,6])\n","## Tensor를 stack으로 연결\n","print(\"x,y,z stack\")\n","print(torch.stack([x,y,z]))\n","print(\"#############\")\n","## stack은 각 Tensor의 0번째 차원을 추가하여 0번째 차원을 이어 붙인 것과 같습니다. 즉 아래 코드와 동일합니다.\n","print(\"concatenate로 구현한 stack\")\n","print(\"각 tensor에 차원 추가 및 추가한 차원으로 연결\")\n","print(torch.cat([x.unsqueeze(0),y.unsqueeze(0),z.unsqueeze(0)],dim=0))\n","print(\"#############\")\n","## dim을 추가한 stack방식\n","print(\"stack dim = 1\")\n","print(torch.stack([x,y,z],dim = 1))\n","# 이것은 마찬가지로 각 텐서에 unsqueeze(1)로 1차원을 추가하고, 해당 차원으로 연결하는 것과 같습니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hos_-WVb50hz","executionInfo":{"status":"ok","timestamp":1739329691569,"user_tz":-540,"elapsed":327,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"bd26ad45-e739-4874-f437-487ca8b55a6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x,y,z stack\n","tensor([[1., 4.],\n","        [2., 5.],\n","        [3., 6.]])\n","#############\n","concatenate로 구현한 stack\n","각 tensor에 차원 추가 및 추가한 차원으로 연결\n","tensor([[1., 4.],\n","        [2., 5.],\n","        [3., 6.]])\n","#############\n","stack dim = 1\n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n"]}]},{"cell_type":"code","source":["# ones_like, zeros_like\n","## ones_like, zeros_like은 제공한 Tensor와 동일한 shape을 가지고 1 또는 0으로 채워진 tensor를 반환합니다.\n","x = torch.FloatTensor([[1,2,3],[9,8,7]])\n","print(\"x\")\n","print(x)\n","print(\"#############\")\n","print(\"ones_like(x)\")\n","print(torch.ones_like(x))\n","print(\"#############\")\n","print(\"zeros_like(x)\")\n","print(torch.zeros_like(x))\n","print(\"#############\")"],"metadata":{"id":"AkYwlW4sCQos","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739329818139,"user_tz":-540,"elapsed":216,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"011f5047-9dfd-4f29-bcd1-45a735c2b851"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x\n","tensor([[1., 2., 3.],\n","        [9., 8., 7.]])\n","#############\n","ones_like(x)\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","#############\n","zeros_like(x)\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","#############\n"]}]},{"cell_type":"code","source":["# Inplace Operation\n","## inplace operation은 계산 결과가 기존 변수에 반영되는 operation(연산)을 말합니다.\n","print(\"Not Inplace Operation\")\n","print(x.add(3))\n","print(\"X after operation\")\n","print(x)\n","print(\"반영되지 않습니다.\")\n","print(\"###################\")\n","print(\"Inplace Operation (adding _ to tail of operation)\")\n","print(x.add_(3))\n","print(\"X after operation\")\n","print(x)\n","print(\"반영이 되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QR5iaSUIu6W","executionInfo":{"status":"ok","timestamp":1739330011035,"user_tz":-540,"elapsed":221,"user":{"displayName":"전길원","userId":"12549511235485585617"}},"outputId":"8f83659c-60d3-43a3-fe31-a9a3f70be1ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Not Inplace Operation\n","tensor([[ 4.,  5.,  6.],\n","        [12., 11., 10.]])\n","X after operation\n","tensor([[1., 2., 3.],\n","        [9., 8., 7.]])\n","반영되지 않습니다.\n","###################\n","Inplace Operation (adding _ to tail of operation)\n","tensor([[ 4.,  5.,  6.],\n","        [12., 11., 10.]])\n","X after operation\n","tensor([[ 4.,  5.,  6.],\n","        [12., 11., 10.]])\n","반영이 되었습니다.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z2Cusf9JJdJp"},"execution_count":null,"outputs":[]}]}